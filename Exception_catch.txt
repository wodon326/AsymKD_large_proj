'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../data/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../data/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../data/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../data/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../data/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../data/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../data/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../data/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../data/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../data/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../data/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../data/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../data/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../data/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../data/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../data/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../data/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../data/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../data/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../data/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../data/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../data/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../data/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../data/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../data/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../data/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../data/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../data/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../data/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../data/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../data/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../data/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/5aaadd4cbc13235570d178a7/blended_images/00000177_masked.jpg../../datasets/AsymKD/BlendedMVS/5aaadd4cbc13235570d178a7/rendered_depth_maps/00000177.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/seasonforest_winter/Hard/P017/image_right/000999_right.png../../datasets/AsymKD/TartanAir/seasonforest_winter/Hard/P017/depth_right/000999_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/5940564ec2d9527ab869f7e2/blended_images/00000229_masked.jpg../../datasets/AsymKD/BlendedMVS/5940564ec2d9527ab869f7e2/rendered_depth_maps/00000229.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/gascola/Easy/P007/image_right/000109_right.png../../datasets/AsymKD/TartanAir/gascola/Easy/P007/depth_right/000109_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/58a44463156b87103d3ed45e/blended_images/00000486_masked.jpg../../datasets/AsymKD/BlendedMVS/58a44463156b87103d3ed45e/rendered_depth_maps/00000486.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/endofworld/Hard/P001/image_right/000006_right.png../../datasets/AsymKD/TartanAir/endofworld/Hard/P001/depth_right/000006_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/hospital/Hard/P042/image_left/000100_left.png../../datasets/AsymKD/TartanAir/hospital/Hard/P042/depth_left/000100_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/59f37f74b45be2233001ba18/blended_images/00000010_masked.jpg../../datasets/AsymKD/BlendedMVS/59f37f74b45be2233001ba18/rendered_depth_maps/00000010.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/gascola/Easy/P007/image_left/001221_left.png../../datasets/AsymKD/TartanAir/gascola/Easy/P007/depth_left/001221_left_depth.npy
CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 1.62 MiB is free. Process 341804 has 8.00 MiB memory in use. Process 341797 has 2.97 GiB memory in use. Process 341800 has 8.00 MiB memory in use. Process 342672 has 224.00 MiB memory in use. Process 342754 has 224.00 MiB memory in use. Process 342874 has 146.00 MiB memory in use. Process 343406 has 224.00 MiB memory in use. Process 343800 has 146.00 MiB memory in use. Process 343721 has 224.00 MiB memory in use. Process 344373 has 224.00 MiB memory in use. Process 344610 has 146.00 MiB memory in use. Process 344728 has 224.00 MiB memory in use. Process 345419 has 224.00 MiB memory in use. Process 345540 has 224.00 MiB memory in use. Process 345580 has 146.00 MiB memory in use. Process 346344 has 224.00 MiB memory in use. Process 346427 has 224.00 MiB memory in use. Process 346626 has 146.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Including non-PyTorch memory, this process has 166.00 MiB memory in use. Process 347556 has 146.00 MiB memory in use. Process 348238 has 224.00 MiB memory in use. Process 348280 has 224.00 MiB memory in use. Process 348563 has 146.00 MiB memory in use. Process 349202 has 224.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 349531 has 146.00 MiB memory in use. Process 350172 has 224.00 MiB memory in use. Process 350215 has 224.00 MiB memory in use. Process 350701 has 146.00 MiB memory in use. Process 351144 has 166.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 351708 has 146.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 352114 has 224.00 MiB memory in use. Process 352678 has 146.00 MiB memory in use. Process 353122 has 224.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 353608 has 146.00 MiB memory in use. Process 354049 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 354578 has 146.00 MiB memory in use. Process 355022 has 224.00 MiB memory in use. Process 355021 has 224.00 MiB memory in use. Process 355589 has 146.00 MiB memory in use. Process 355988 has 224.00 MiB memory in use. Process 356029 has 224.00 MiB memory in use. Process 356517 has 146.00 MiB memory in use. Process 356998 has 224.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Of the allocated memory 20.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/BlendedMVS/585559d9804be10585238ddf/blended_images/00000163_masked.jpg../../datasets/AsymKD/BlendedMVS/585559d9804be10585238ddf/rendered_depth_maps/00000163.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/seasidetown/Easy/P005/image_left/000381_left.png../../datasets/AsymKD/TartanAir/seasidetown/Easy/P005/depth_left/000381_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/5895d38f9a8c0314c5cfe50c/blended_images/00000011_masked.jpg../../datasets/AsymKD/BlendedMVS/5895d38f9a8c0314c5cfe50c/rendered_depth_maps/00000011.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/5b558a928bbfb62204e77ba2/blended_images/00000026_masked.jpg../../datasets/AsymKD/BlendedMVS/5b558a928bbfb62204e77ba2/rendered_depth_maps/00000026.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/58d36897f387231e6c929903/blended_images/00000006_masked.jpg../../datasets/AsymKD/BlendedMVS/58d36897f387231e6c929903/rendered_depth_maps/00000006.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/endofworld/Easy/P009/image_left/000457_left.png../../datasets/AsymKD/TartanAir/endofworld/Easy/P009/depth_left/000457_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/59ecfd02e225f6492d20fcc9/blended_images/00000044_masked.jpg../../datasets/AsymKD/BlendedMVS/59ecfd02e225f6492d20fcc9/rendered_depth_maps/00000044.pfm
CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 1.62 MiB is free. Process 341804 has 8.00 MiB memory in use. Process 341797 has 2.97 GiB memory in use. Process 341800 has 8.00 MiB memory in use. Process 342672 has 224.00 MiB memory in use. Process 342754 has 224.00 MiB memory in use. Process 342874 has 146.00 MiB memory in use. Process 343406 has 224.00 MiB memory in use. Process 343800 has 146.00 MiB memory in use. Process 343721 has 224.00 MiB memory in use. Process 344373 has 224.00 MiB memory in use. Process 344610 has 146.00 MiB memory in use. Process 344728 has 224.00 MiB memory in use. Process 345419 has 224.00 MiB memory in use. Process 345540 has 224.00 MiB memory in use. Process 345580 has 146.00 MiB memory in use. Process 346344 has 224.00 MiB memory in use. Process 346427 has 224.00 MiB memory in use. Process 346626 has 146.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 347352 has 166.00 MiB memory in use. Process 347556 has 146.00 MiB memory in use. Process 348238 has 224.00 MiB memory in use. Process 348280 has 224.00 MiB memory in use. Process 348563 has 146.00 MiB memory in use. Process 349202 has 224.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 349531 has 146.00 MiB memory in use. Process 350172 has 224.00 MiB memory in use. Process 350215 has 224.00 MiB memory in use. Process 350701 has 146.00 MiB memory in use. Including non-PyTorch memory, this process has 166.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 351708 has 146.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 352114 has 224.00 MiB memory in use. Process 352678 has 146.00 MiB memory in use. Process 353122 has 224.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 353608 has 146.00 MiB memory in use. Process 354049 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 354578 has 146.00 MiB memory in use. Process 355022 has 224.00 MiB memory in use. Process 355021 has 224.00 MiB memory in use. Process 355589 has 146.00 MiB memory in use. Process 355988 has 224.00 MiB memory in use. Process 356029 has 224.00 MiB memory in use. Process 356517 has 146.00 MiB memory in use. Process 356998 has 224.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Of the allocated memory 20.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/gascola/Easy/P003/image_right/001207_right.png../../datasets/AsymKD/TartanAir/gascola/Easy/P003/depth_right/001207_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/hospital/Easy/P001/image_right/000236_right.png../../datasets/AsymKD/TartanAir/hospital/Easy/P001/depth_right/000236_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/5880e3422366dd5d06e5ff8e/blended_images/00000113_masked.jpg../../datasets/AsymKD/BlendedMVS/5880e3422366dd5d06e5ff8e/rendered_depth_maps/00000113.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/seasonforest/Easy/P010/image_left/000317_left.png../../datasets/AsymKD/TartanAir/seasonforest/Easy/P010/depth_left/000317_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/ocean/Easy/P008/image_left/000084_left.png../../datasets/AsymKD/TartanAir/ocean/Easy/P008/depth_left/000084_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/HRWSI/train/imgs/112099_Matran_W3D.jpg../../datasets/AsymKD/HRWSI/train/gts/112099_Matran_W3D.png
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory/Hard/P008/image_left/000632_left.png../../datasets/AsymKD/TartanAir/abandonedfactory/Hard/P008/depth_left/000632_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P010/image_right/000058_right.png../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P010/depth_right/000058_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/westerndesert/Hard/P005/image_left/000568_left.png../../datasets/AsymKD/TartanAir/westerndesert/Hard/P005/depth_left/000568_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/oldtown/Hard/P008/image_left/000052_left.png../../datasets/AsymKD/TartanAir/oldtown/Hard/P008/depth_left/000052_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/59c1c3e2fd6e3d4ead9f1013/blended_images/00001196_masked.jpg../../datasets/AsymKD/BlendedMVS/59c1c3e2fd6e3d4ead9f1013/rendered_depth_maps/00001196.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/oldtown/Easy/P001/image_right/000101_right.png../../datasets/AsymKD/TartanAir/oldtown/Easy/P001/depth_right/000101_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/586326ad712e276146904571/blended_images/00000031_masked.jpg../../datasets/AsymKD/BlendedMVS/586326ad712e276146904571/rendered_depth_maps/00000031.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/5a6b1c418d100c2f8fdc4411/blended_images/00000784_masked.jpg../../datasets/AsymKD/BlendedMVS/5a6b1c418d100c2f8fdc4411/rendered_depth_maps/00000784.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory_night/Hard/P005/image_right/000477_right.png../../datasets/AsymKD/TartanAir/abandonedfactory_night/Hard/P005/depth_right/000477_right_depth.npy
CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 1.62 MiB is free. Process 341804 has 8.00 MiB memory in use. Process 341797 has 2.97 GiB memory in use. Process 341800 has 8.00 MiB memory in use. Process 342672 has 224.00 MiB memory in use. Process 342754 has 224.00 MiB memory in use. Process 342874 has 146.00 MiB memory in use. Process 343406 has 224.00 MiB memory in use. Process 343800 has 146.00 MiB memory in use. Process 343721 has 224.00 MiB memory in use. Process 344373 has 224.00 MiB memory in use. Process 344610 has 146.00 MiB memory in use. Process 344728 has 224.00 MiB memory in use. Process 345419 has 224.00 MiB memory in use. Process 345540 has 224.00 MiB memory in use. Process 345580 has 146.00 MiB memory in use. Process 346344 has 224.00 MiB memory in use. Process 346427 has 224.00 MiB memory in use. Process 346626 has 146.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 347352 has 166.00 MiB memory in use. Process 347556 has 146.00 MiB memory in use. Process 348238 has 224.00 MiB memory in use. Process 348280 has 224.00 MiB memory in use. Process 348563 has 146.00 MiB memory in use. Process 349202 has 224.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 349531 has 146.00 MiB memory in use. Process 350172 has 224.00 MiB memory in use. Process 350215 has 224.00 MiB memory in use. Process 350701 has 146.00 MiB memory in use. Process 351144 has 166.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 351708 has 146.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 352114 has 224.00 MiB memory in use. Process 352678 has 146.00 MiB memory in use. Process 353122 has 224.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 353608 has 146.00 MiB memory in use. Process 354049 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 354578 has 146.00 MiB memory in use. Process 355022 has 224.00 MiB memory in use. Process 355021 has 224.00 MiB memory in use. Process 355589 has 146.00 MiB memory in use. Process 355988 has 224.00 MiB memory in use. Process 356029 has 224.00 MiB memory in use. Process 356517 has 146.00 MiB memory in use. Process 356998 has 224.00 MiB memory in use. Including non-PyTorch memory, this process has 166.00 MiB memory in use. Of the allocated memory 20.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/HRWSI/train/imgs/Kalahari-16.jpg../../datasets/AsymKD/HRWSI/train/gts/Kalahari-16.png
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/oldtown/Hard/P002/image_left/000310_left.png../../datasets/AsymKD/TartanAir/oldtown/Hard/P002/depth_left/000310_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/584bdadf7072670e72c0005c/blended_images/00000046_masked.jpg../../datasets/AsymKD/BlendedMVS/584bdadf7072670e72c0005c/rendered_depth_maps/00000046.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory_night/Easy/P011/image_left/001537_left.png../../datasets/AsymKD/TartanAir/abandonedfactory_night/Easy/P011/depth_left/001537_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/HRWSI/train/imgs/81892_z98_JodhpurMehrangarhFort.jpg../../datasets/AsymKD/HRWSI/train/gts/81892_z98_JodhpurMehrangarhFort.png
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/gascola/Easy/P003/image_right/001256_right.png../../datasets/AsymKD/TartanAir/gascola/Easy/P003/depth_right/001256_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/586133c2712e2761468ecfe3/blended_images/00000233_masked.jpg../../datasets/AsymKD/BlendedMVS/586133c2712e2761468ecfe3/rendered_depth_maps/00000233.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/amusement/Hard/P005/image_right/000690_right.png../../datasets/AsymKD/TartanAir/amusement/Hard/P005/depth_right/000690_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/589388059a8c0314c5ce718b/blended_images/00000239_masked.jpg../../datasets/AsymKD/BlendedMVS/589388059a8c0314c5ce718b/rendered_depth_maps/00000239.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/588857f6c02346100f4ac09f/blended_images/00000168_masked.jpg../../datasets/AsymKD/BlendedMVS/588857f6c02346100f4ac09f/rendered_depth_maps/00000168.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/586281d2712e2761468fcaa2/blended_images/00000070_masked.jpg../../datasets/AsymKD/BlendedMVS/586281d2712e2761468fcaa2/rendered_depth_maps/00000070.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/5bea87f4abd34c35e1860ab5/blended_images/00000149_masked.jpg../../datasets/AsymKD/BlendedMVS/5bea87f4abd34c35e1860ab5/rendered_depth_maps/00000149.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/hospital/Easy/P013/image_left/000309_left.png../../datasets/AsymKD/TartanAir/hospital/Easy/P013/depth_left/000309_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/seasonforest_winter/Hard/P017/image_left/000307_left.png../../datasets/AsymKD/TartanAir/seasonforest_winter/Hard/P017/depth_left/000307_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/carwelding/Hard/P001/image_right/000275_right.png../../datasets/AsymKD/TartanAir/carwelding/Hard/P001/depth_right/000275_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/588e35e690414422fbe90a53/blended_images/00000301_masked.jpg../../datasets/AsymKD/BlendedMVS/588e35e690414422fbe90a53/rendered_depth_maps/00000301.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/gascola/Hard/P006/image_right/000080_right.png../../datasets/AsymKD/TartanAir/gascola/Hard/P006/depth_right/000080_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/seasonforest/Easy/P010/image_right/000400_right.png../../datasets/AsymKD/TartanAir/seasonforest/Easy/P010/depth_right/000400_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/ocean/Easy/P009/image_left/000343_left.png../../datasets/AsymKD/TartanAir/ocean/Easy/P009/depth_left/000343_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory/Hard/P004/image_right/000805_right.png../../datasets/AsymKD/TartanAir/abandonedfactory/Hard/P004/depth_right/000805_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/office2/Hard/P010/image_left/000365_left.png../../datasets/AsymKD/TartanAir/office2/Hard/P010/depth_left/000365_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/5bbb6eb2ea1cfa39f1af7e0c/blended_images/00000131_masked.jpg../../datasets/AsymKD/BlendedMVS/5bbb6eb2ea1cfa39f1af7e0c/rendered_depth_maps/00000131.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/HRWSI/train/imgs/80903_z98_AgraFort.jpg../../datasets/AsymKD/HRWSI/train/gts/80903_z98_AgraFort.png
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/hospital/Easy/P025/image_right/000005_right.png../../datasets/AsymKD/TartanAir/hospital/Easy/P025/depth_right/000005_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/589145ef90414422fbeb2e08/blended_images/00000059_masked.jpg../../datasets/AsymKD/BlendedMVS/589145ef90414422fbeb2e08/rendered_depth_maps/00000059.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory_night/Hard/P010/image_right/000170_right.png../../datasets/AsymKD/TartanAir/abandonedfactory_night/Hard/P010/depth_right/000170_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/japanesealley/Hard/P005/image_right/000360_right.png../../datasets/AsymKD/TartanAir/japanesealley/Hard/P005/depth_right/000360_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory/Hard/P009/image_left/000519_left.png../../datasets/AsymKD/TartanAir/abandonedfactory/Hard/P009/depth_left/000519_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/586133c2712e2761468ecfe3/blended_images/00000267_masked.jpg../../datasets/AsymKD/BlendedMVS/586133c2712e2761468ecfe3/rendered_depth_maps/00000267.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/westerndesert/Hard/P004/image_left/000120_left.png../../datasets/AsymKD/TartanAir/westerndesert/Hard/P004/depth_left/000120_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/oldtown/Easy/P002/image_left/001632_left.png../../datasets/AsymKD/TartanAir/oldtown/Easy/P002/depth_left/001632_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/586b0e219d1b5e34c2828862/blended_images/00000122_masked.jpg../../datasets/AsymKD/BlendedMVS/586b0e219d1b5e34c2828862/rendered_depth_maps/00000122.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/seasonforest_winter/Easy/P005/image_left/001373_left.png../../datasets/AsymKD/TartanAir/seasonforest_winter/Easy/P005/depth_left/001373_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/ocean/Easy/P008/image_left/001147_left.png../../datasets/AsymKD/TartanAir/ocean/Easy/P008/depth_left/001147_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/oldtown/Easy/P002/image_left/001248_left.png../../datasets/AsymKD/TartanAir/oldtown/Easy/P002/depth_left/001248_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P011/image_left/001719_left.png../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P011/depth_left/001719_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/amusement/Hard/P006/image_left/000410_left.png../../datasets/AsymKD/TartanAir/amusement/Hard/P006/depth_left/000410_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/gascola/Easy/P004/image_right/000091_right.png../../datasets/AsymKD/TartanAir/gascola/Easy/P004/depth_right/000091_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/hospital/Easy/P025/image_right/000015_right.png../../datasets/AsymKD/TartanAir/hospital/Easy/P025/depth_right/000015_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/HRWSI/train/imgs/83995_z98_LausanneMuseeZoologieAnimalGrandTanrec-Herisson.jpg../../datasets/AsymKD/HRWSI/train/gts/83995_z98_LausanneMuseeZoologieAnimalGrandTanrec-Herisson.png
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory/Hard/P009/image_left/001825_left.png../../datasets/AsymKD/TartanAir/abandonedfactory/Hard/P009/depth_left/001825_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/HRWSI/train/imgs/107213_SpainAndalousiaCadix_W3D.jpg../../datasets/AsymKD/HRWSI/train/gts/107213_SpainAndalousiaCadix_W3D.png
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/oldtown/Easy/P001/image_left/001371_left.png../../datasets/AsymKD/TartanAir/oldtown/Easy/P001/depth_left/001371_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/ocean/Hard/P006/image_right/000296_right.png../../datasets/AsymKD/TartanAir/ocean/Hard/P006/depth_right/000296_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/hospital/Hard/P047/image_left/000305_left.png../../datasets/AsymKD/TartanAir/hospital/Hard/P047/depth_left/000305_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/office2/Easy/P008/image_right/001412_right.png../../datasets/AsymKD/TartanAir/office2/Easy/P008/depth_right/001412_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/japanesealley/Hard/P001/image_right/000233_right.png../../datasets/AsymKD/TartanAir/japanesealley/Hard/P001/depth_right/000233_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/office/Easy/P002/image_left/001150_left.png../../datasets/AsymKD/TartanAir/office/Easy/P002/depth_left/001150_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/hospital/Easy/P003/image_left/000444_left.png../../datasets/AsymKD/TartanAir/hospital/Easy/P003/depth_left/000444_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory_night/Easy/P008/image_left/001691_left.png../../datasets/AsymKD/TartanAir/abandonedfactory_night/Easy/P008/depth_left/001691_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/gascola/Hard/P002/image_left/001209_left.png../../datasets/AsymKD/TartanAir/gascola/Hard/P002/depth_left/001209_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/5aa0f478a9efce63548c1cb4/blended_images/00000036_masked.jpg../../datasets/AsymKD/BlendedMVS/5aa0f478a9efce63548c1cb4/rendered_depth_maps/00000036.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/seasonforest_winter/Easy/P005/image_right/000425_right.png../../datasets/AsymKD/TartanAir/seasonforest_winter/Easy/P005/depth_right/000425_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory_night/Easy/P010/image_right/000114_right.png../../datasets/AsymKD/TartanAir/abandonedfactory_night/Easy/P010/depth_right/000114_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/587f365d2366dd5d06e4906e/blended_images/00000364_masked.jpg../../datasets/AsymKD/BlendedMVS/587f365d2366dd5d06e4906e/rendered_depth_maps/00000364.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/seasidetown/Easy/P009/image_left/000422_left.png../../datasets/AsymKD/TartanAir/seasidetown/Easy/P009/depth_left/000422_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/58873d44b791366d617a65dd/blended_images/00000075_masked.jpg../../datasets/AsymKD/BlendedMVS/58873d44b791366d617a65dd/rendered_depth_maps/00000075.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/5b60fa0c764f146feef84df0/blended_images/00000079_masked.jpg../../datasets/AsymKD/BlendedMVS/5b60fa0c764f146feef84df0/rendered_depth_maps/00000079.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/carwelding/Hard/P002/image_right/000113_right.png../../datasets/AsymKD/TartanAir/carwelding/Hard/P002/depth_right/000113_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P010/image_left/000014_left.png../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P010/depth_left/000014_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/seasidetown/Easy/P008/image_left/000159_left.png../../datasets/AsymKD/TartanAir/seasidetown/Easy/P008/depth_left/000159_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/585bbe55c49c8507c3ce81cd/blended_images/00000287_masked.jpg../../datasets/AsymKD/BlendedMVS/585bbe55c49c8507c3ce81cd/rendered_depth_maps/00000287.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P011/image_right/001324_right.png../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P011/depth_right/001324_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/588e01c490414422fbe8ee2a/blended_images/00000070_masked.jpg../../datasets/AsymKD/BlendedMVS/588e01c490414422fbe8ee2a/rendered_depth_maps/00000070.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/HRWSI/train/imgs/114589_ItaliaVaticanPlaceSt-Pierre_W3D.jpg../../datasets/AsymKD/HRWSI/train/gts/114589_ItaliaVaticanPlaceSt-Pierre_W3D.png
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/seasidetown/Easy/P001/image_right/000083_right.png../../datasets/AsymKD/TartanAir/seasidetown/Easy/P001/depth_right/000083_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/office/Easy/P002/image_left/001389_left.png../../datasets/AsymKD/TartanAir/office/Easy/P002/depth_left/001389_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/hospital/Easy/P008/image_right/000031_right.png../../datasets/AsymKD/TartanAir/hospital/Easy/P008/depth_right/000031_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/endofworld/Easy/P003/image_left/000107_left.png../../datasets/AsymKD/TartanAir/endofworld/Easy/P003/depth_left/000107_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/ocean/Hard/P000/image_right/000952_right.png../../datasets/AsymKD/TartanAir/ocean/Hard/P000/depth_right/000952_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/59056e6760bb961de55f3501/blended_images/00000195_masked.jpg../../datasets/AsymKD/BlendedMVS/59056e6760bb961de55f3501/rendered_depth_maps/00000195.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/carwelding/Hard/P002/image_left/000184_left.png../../datasets/AsymKD/TartanAir/carwelding/Hard/P002/depth_left/000184_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/ocean/Hard/P004/image_right/000010_right.png../../datasets/AsymKD/TartanAir/ocean/Hard/P004/depth_right/000010_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/hospital/Easy/P017/image_left/000145_left.png../../datasets/AsymKD/TartanAir/hospital/Easy/P017/depth_left/000145_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/oldtown/Hard/P002/image_right/000755_right.png../../datasets/AsymKD/TartanAir/oldtown/Hard/P002/depth_right/000755_right_depth.npy
CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 5.62 MiB is free. Process 341797 has 2.97 GiB memory in use. Process 342672 has 224.00 MiB memory in use. Process 342754 has 224.00 MiB memory in use. Process 342874 has 146.00 MiB memory in use. Process 343406 has 224.00 MiB memory in use. Process 343800 has 146.00 MiB memory in use. Process 343721 has 224.00 MiB memory in use. Process 344373 has 224.00 MiB memory in use. Process 344610 has 146.00 MiB memory in use. Process 344728 has 224.00 MiB memory in use. Process 345419 has 224.00 MiB memory in use. Process 345540 has 224.00 MiB memory in use. Process 345580 has 146.00 MiB memory in use. Process 346344 has 224.00 MiB memory in use. Process 346427 has 224.00 MiB memory in use. Process 346626 has 146.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 347352 has 166.00 MiB memory in use. Process 347556 has 146.00 MiB memory in use. Process 348238 has 224.00 MiB memory in use. Process 348280 has 224.00 MiB memory in use. Process 348563 has 146.00 MiB memory in use. Process 349202 has 224.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 349531 has 146.00 MiB memory in use. Process 350172 has 224.00 MiB memory in use. Process 350215 has 224.00 MiB memory in use. Process 350701 has 146.00 MiB memory in use. Process 351144 has 166.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 351708 has 146.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 352114 has 224.00 MiB memory in use. Process 352678 has 146.00 MiB memory in use. Process 353122 has 224.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 353608 has 146.00 MiB memory in use. Process 354049 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 354578 has 146.00 MiB memory in use. Process 355022 has 224.00 MiB memory in use. Process 355021 has 224.00 MiB memory in use. Process 355589 has 146.00 MiB memory in use. Process 355988 has 224.00 MiB memory in use. Process 356029 has 224.00 MiB memory in use. Process 356517 has 146.00 MiB memory in use. Process 356998 has 224.00 MiB memory in use. Including non-PyTorch memory, this process has 166.00 MiB memory in use. Of the allocated memory 20.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/endofworld/Easy/P009/image_right/000500_right.png../../datasets/AsymKD/TartanAir/endofworld/Easy/P009/depth_right/000500_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/oldtown/Easy/P007/image_right/001306_right.png../../datasets/AsymKD/TartanAir/oldtown/Easy/P007/depth_right/001306_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/hospital/Hard/P047/image_left/000515_left.png../../datasets/AsymKD/TartanAir/hospital/Hard/P047/depth_left/000515_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/58800b0b2366dd5d06e5312d/blended_images/00000397_masked.jpg../../datasets/AsymKD/BlendedMVS/58800b0b2366dd5d06e5312d/rendered_depth_maps/00000397.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/hospital/Hard/P043/image_right/000579_right.png../../datasets/AsymKD/TartanAir/hospital/Hard/P043/depth_right/000579_right_depth.npy
CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 5.62 MiB is free. Process 341797 has 2.97 GiB memory in use. Process 342672 has 224.00 MiB memory in use. Process 342754 has 224.00 MiB memory in use. Process 342874 has 146.00 MiB memory in use. Process 343406 has 224.00 MiB memory in use. Process 343800 has 146.00 MiB memory in use. Process 343721 has 224.00 MiB memory in use. Process 344373 has 224.00 MiB memory in use. Process 344610 has 146.00 MiB memory in use. Process 344728 has 224.00 MiB memory in use. Process 345419 has 224.00 MiB memory in use. Process 345540 has 224.00 MiB memory in use. Process 345580 has 146.00 MiB memory in use. Process 346344 has 224.00 MiB memory in use. Process 346427 has 224.00 MiB memory in use. Process 346626 has 146.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Including non-PyTorch memory, this process has 166.00 MiB memory in use. Process 347556 has 146.00 MiB memory in use. Process 348238 has 224.00 MiB memory in use. Process 348280 has 224.00 MiB memory in use. Process 348563 has 146.00 MiB memory in use. Process 349202 has 224.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 349531 has 146.00 MiB memory in use. Process 350172 has 224.00 MiB memory in use. Process 350215 has 224.00 MiB memory in use. Process 350701 has 146.00 MiB memory in use. Process 351144 has 166.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 351708 has 146.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 352114 has 224.00 MiB memory in use. Process 352678 has 146.00 MiB memory in use. Process 353122 has 224.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 353608 has 146.00 MiB memory in use. Process 354049 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 354578 has 146.00 MiB memory in use. Process 355022 has 224.00 MiB memory in use. Process 355021 has 224.00 MiB memory in use. Process 355589 has 146.00 MiB memory in use. Process 355988 has 224.00 MiB memory in use. Process 356029 has 224.00 MiB memory in use. Process 356517 has 146.00 MiB memory in use. Process 356998 has 224.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Of the allocated memory 20.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/office2/Hard/P003/image_right/001283_right.png../../datasets/AsymKD/TartanAir/office2/Hard/P003/depth_right/001283_right_depth.npy
CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 5.62 MiB is free. Process 341797 has 2.97 GiB memory in use. Process 342672 has 224.00 MiB memory in use. Process 342754 has 224.00 MiB memory in use. Process 342874 has 146.00 MiB memory in use. Process 343406 has 224.00 MiB memory in use. Process 343800 has 146.00 MiB memory in use. Process 343721 has 224.00 MiB memory in use. Process 344373 has 224.00 MiB memory in use. Process 344610 has 146.00 MiB memory in use. Process 344728 has 224.00 MiB memory in use. Process 345419 has 224.00 MiB memory in use. Process 345540 has 224.00 MiB memory in use. Process 345580 has 146.00 MiB memory in use. Process 346344 has 224.00 MiB memory in use. Process 346427 has 224.00 MiB memory in use. Process 346626 has 146.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 347352 has 166.00 MiB memory in use. Process 347556 has 146.00 MiB memory in use. Process 348238 has 224.00 MiB memory in use. Process 348280 has 224.00 MiB memory in use. Process 348563 has 146.00 MiB memory in use. Process 349202 has 224.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 349531 has 146.00 MiB memory in use. Process 350172 has 224.00 MiB memory in use. Process 350215 has 224.00 MiB memory in use. Process 350701 has 146.00 MiB memory in use. Including non-PyTorch memory, this process has 166.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 351708 has 146.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 352114 has 224.00 MiB memory in use. Process 352678 has 146.00 MiB memory in use. Process 353122 has 224.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 353608 has 146.00 MiB memory in use. Process 354049 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 354578 has 146.00 MiB memory in use. Process 355022 has 224.00 MiB memory in use. Process 355021 has 224.00 MiB memory in use. Process 355589 has 146.00 MiB memory in use. Process 355988 has 224.00 MiB memory in use. Process 356029 has 224.00 MiB memory in use. Process 356517 has 146.00 MiB memory in use. Process 356998 has 224.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Of the allocated memory 20.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/hospital/Hard/P046/image_left/000121_left.png../../datasets/AsymKD/TartanAir/hospital/Hard/P046/depth_left/000121_left_depth.npy
CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 7.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359689 has 34.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359692 has 38.00 MiB memory in use. Process 359851 has 166.00 MiB memory in use. Process 360657 has 38.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 166.00 MiB memory in use. Process 360739 has 146.00 MiB memory in use. Process 361352 has 38.00 MiB memory in use. Process 361866 has 166.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 361709 has 34.00 MiB memory in use. Process 362316 has 34.00 MiB memory in use. Process 362714 has 38.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 224.00 MiB memory in use. Process 363602 has 146.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364238 has 38.00 MiB memory in use. Process 364611 has 146.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 166.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365660 has 38.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 365899 has 34.00 MiB memory in use. Process 366588 has 166.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 166.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 146.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 166.00 MiB memory in use. Process 368249 has 224.00 MiB memory in use. Process 368526 has 38.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369207 has 38.00 MiB memory in use. Process 369532 has 38.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 369813 has 34.00 MiB memory in use. Process 370128 has 38.00 MiB memory in use. Process 370299 has 38.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Including non-PyTorch memory, this process has 146.00 MiB memory in use. Process 371263 has 34.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371831 has 34.00 MiB memory in use. Process 371872 has 38.00 MiB memory in use. Process 372147 has 166.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372840 has 34.00 MiB memory in use. Process 372880 has 146.00 MiB memory in use. Process 372920 has 34.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373808 has 38.00 MiB memory in use. Process 373888 has 38.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/seasonforest_winter/Hard/P017/image_left/000307_left.png../../datasets/AsymKD/TartanAir/seasonforest_winter/Hard/P017/depth_left/000307_left_depth.npy
CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 5.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359689 has 34.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359692 has 38.00 MiB memory in use. Process 359851 has 166.00 MiB memory in use. Process 360657 has 34.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 166.00 MiB memory in use. Process 360739 has 146.00 MiB memory in use. Process 361352 has 34.00 MiB memory in use. Process 361866 has 198.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 361709 has 12.00 MiB memory in use. Process 362316 has 12.00 MiB memory in use. Process 362714 has 34.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 224.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364238 has 34.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 166.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365660 has 36.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 365899 has 12.00 MiB memory in use. Process 366588 has 166.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 166.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Including non-PyTorch memory, this process has 146.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 166.00 MiB memory in use. Process 368249 has 224.00 MiB memory in use. Process 368526 has 38.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369207 has 34.00 MiB memory in use. Process 369532 has 34.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 369813 has 34.00 MiB memory in use. Process 370128 has 34.00 MiB memory in use. Process 370299 has 38.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 146.00 MiB memory in use. Process 371263 has 34.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371831 has 34.00 MiB memory in use. Process 371872 has 38.00 MiB memory in use. Process 372147 has 166.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372840 has 12.00 MiB memory in use. Process 372880 has 146.00 MiB memory in use. Process 372920 has 34.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373808 has 38.00 MiB memory in use. Process 373888 has 38.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/HRWSI/train/imgs/103371_z98_CretDuLocleCrecheDeTousLesSensBianchiCalame.jpg../../datasets/AsymKD/HRWSI/train/gts/103371_z98_CretDuLocleCrecheDeTousLesSensBianchiCalame.png
CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 9.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359689 has 12.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359692 has 34.00 MiB memory in use. Process 359851 has 166.00 MiB memory in use. Process 360657 has 34.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 166.00 MiB memory in use. Process 360739 has 146.00 MiB memory in use. Process 361352 has 34.00 MiB memory in use. Process 361866 has 200.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 361709 has 12.00 MiB memory in use. Process 362316 has 12.00 MiB memory in use. Process 362714 has 34.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364238 has 34.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Including non-PyTorch memory, this process has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 166.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365660 has 34.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 365899 has 12.00 MiB memory in use. Process 366588 has 166.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 166.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 146.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 166.00 MiB memory in use. Process 368249 has 224.00 MiB memory in use. Process 368526 has 38.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369207 has 34.00 MiB memory in use. Process 369532 has 34.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 369813 has 12.00 MiB memory in use. Process 370128 has 34.00 MiB memory in use. Process 370299 has 34.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 146.00 MiB memory in use. Process 371263 has 12.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371831 has 34.00 MiB memory in use. Process 371872 has 146.00 MiB memory in use. Process 372147 has 178.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372840 has 12.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 372920 has 12.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373808 has 34.00 MiB memory in use. Process 373888 has 34.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/seasonforest_winter/Easy/P007/image_right/000313_right.png../../datasets/AsymKD/TartanAir/seasonforest_winter/Easy/P007/depth_right/000313_right_depth.npy
CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 73.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359689 has 12.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359692 has 12.00 MiB memory in use. Process 359851 has 166.00 MiB memory in use. Process 360657 has 12.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 198.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361352 has 12.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 361709 has 12.00 MiB memory in use. Process 362316 has 8.00 MiB memory in use. Process 362714 has 12.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Including non-PyTorch memory, this process has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364238 has 12.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 166.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365660 has 12.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 365899 has 8.00 MiB memory in use. Process 366588 has 166.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 166.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 166.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368526 has 34.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369207 has 12.00 MiB memory in use. Process 369532 has 12.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 369813 has 12.00 MiB memory in use. Process 370128 has 12.00 MiB memory in use. Process 370299 has 14.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 146.00 MiB memory in use. Process 371263 has 12.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371831 has 12.00 MiB memory in use. Process 371872 has 146.00 MiB memory in use. Process 372147 has 178.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372840 has 8.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 372920 has 12.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373808 has 34.00 MiB memory in use. Process 373888 has 34.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/gascola/Easy/P003/image_right/001256_right.png../../datasets/AsymKD/TartanAir/gascola/Easy/P003/depth_right/001256_right_depth.npy
CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 33.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359689 has 10.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359692 has 12.00 MiB memory in use. Process 359851 has 166.00 MiB memory in use. Process 360657 has 12.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 200.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361352 has 12.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 361709 has 8.00 MiB memory in use. Process 362316 has 8.00 MiB memory in use. Process 362714 has 12.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364238 has 12.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 166.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365660 has 12.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 365899 has 8.00 MiB memory in use. Process 366588 has 166.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 166.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 178.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368526 has 14.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369207 has 12.00 MiB memory in use. Process 369532 has 12.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 369813 has 12.00 MiB memory in use. Process 370128 has 12.00 MiB memory in use. Process 370299 has 12.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Including non-PyTorch memory, this process has 146.00 MiB memory in use. Process 370902 has 146.00 MiB memory in use. Process 371263 has 10.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371831 has 12.00 MiB memory in use. Process 371872 has 146.00 MiB memory in use. Process 372147 has 178.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372840 has 8.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 372920 has 8.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373808 has 12.00 MiB memory in use. Process 373888 has 12.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/abandonedfactory_night/Hard/P007/image_left/000244_left.png../../datasets/AsymKD/TartanAir/abandonedfactory_night/Hard/P007/depth_left/000244_left_depth.npy
CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 7.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359689 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359692 has 12.00 MiB memory in use. Process 359851 has 166.00 MiB memory in use. Process 360657 has 8.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361352 has 8.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 361709 has 8.00 MiB memory in use. Process 362316 has 8.00 MiB memory in use. Process 362714 has 8.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364238 has 8.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 200.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365660 has 12.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 365899 has 8.00 MiB memory in use. Process 366588 has 198.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 198.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368526 has 12.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369207 has 8.00 MiB memory in use. Process 369532 has 12.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 369813 has 8.00 MiB memory in use. Process 370128 has 8.00 MiB memory in use. Process 370299 has 12.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371263 has 8.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371831 has 8.00 MiB memory in use. Process 371872 has 146.00 MiB memory in use. Including non-PyTorch memory, this process has 178.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372840 has 8.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 372920 has 8.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373808 has 12.00 MiB memory in use. Process 373888 has 12.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 32.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P010/image_left/001368_left.png../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P010/depth_left/001368_left_depth.npy
CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 3.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359689 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359692 has 8.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360657 has 8.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361352 has 8.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 361709 has 8.00 MiB memory in use. Process 362316 has 8.00 MiB memory in use. Process 362714 has 8.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Including non-PyTorch memory, this process has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364238 has 8.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365660 has 8.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 365899 has 8.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368526 has 8.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369207 has 8.00 MiB memory in use. Process 369532 has 8.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 369813 has 8.00 MiB memory in use. Process 370128 has 8.00 MiB memory in use. Process 370299 has 8.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371263 has 8.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371831 has 8.00 MiB memory in use. Process 371872 has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372840 has 8.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 372920 has 8.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373808 has 8.00 MiB memory in use. Process 373888 has 8.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P011/image_right/001324_right.png../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P011/depth_right/001324_right_depth.npy
CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 3.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359689 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359692 has 8.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360657 has 8.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361352 has 8.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 361709 has 8.00 MiB memory in use. Process 362316 has 8.00 MiB memory in use. Process 362714 has 8.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364238 has 8.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Including non-PyTorch memory, this process has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365660 has 8.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 365899 has 8.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368526 has 8.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369207 has 8.00 MiB memory in use. Process 369532 has 8.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 369813 has 8.00 MiB memory in use. Process 370128 has 8.00 MiB memory in use. Process 370299 has 8.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371263 has 8.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371831 has 8.00 MiB memory in use. Process 371872 has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372840 has 8.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 372920 has 8.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373808 has 8.00 MiB memory in use. Process 373888 has 8.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/BlendedMVS/585559d9804be10585238ddf/blended_images/00000163_masked.jpg../../datasets/AsymKD/BlendedMVS/585559d9804be10585238ddf/rendered_depth_maps/00000163.pfm
CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 3.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359689 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359692 has 8.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360657 has 8.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361352 has 8.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 361709 has 8.00 MiB memory in use. Process 362316 has 8.00 MiB memory in use. Process 362714 has 8.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364238 has 8.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365660 has 8.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 365899 has 8.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368526 has 8.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Including non-PyTorch memory, this process has 166.00 MiB memory in use. Process 369207 has 8.00 MiB memory in use. Process 369532 has 8.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 369813 has 8.00 MiB memory in use. Process 370128 has 8.00 MiB memory in use. Process 370299 has 8.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371263 has 8.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371831 has 8.00 MiB memory in use. Process 371872 has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372840 has 8.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 372920 has 8.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373808 has 8.00 MiB memory in use. Process 373888 has 8.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 20.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/BlendedMVS/000000000000000000000008/blended_images/00000191_masked.jpg../../datasets/AsymKD/BlendedMVS/000000000000000000000008/rendered_depth_maps/00000191.pfm
CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 3.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359689 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359692 has 8.00 MiB memory in use. Including non-PyTorch memory, this process has 178.00 MiB memory in use. Process 360657 has 8.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361352 has 8.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 362714 has 8.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364238 has 8.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365660 has 8.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 365899 has 8.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368526 has 8.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369207 has 8.00 MiB memory in use. Process 369532 has 8.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 369813 has 8.00 MiB memory in use. Process 370128 has 8.00 MiB memory in use. Process 370299 has 8.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371263 has 8.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371831 has 8.00 MiB memory in use. Process 371872 has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 372920 has 8.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373808 has 8.00 MiB memory in use. Process 373888 has 8.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 32.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/HRWSI/train/imgs/84531_z98_RussiaMoscow.jpg../../datasets/AsymKD/HRWSI/train/gts/84531_z98_RussiaMoscow.png
CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 3.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359689 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359692 has 8.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360657 has 8.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361352 has 8.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 362714 has 8.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364238 has 8.00 MiB memory in use. Including non-PyTorch memory, this process has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365660 has 8.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 365899 has 8.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368526 has 8.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369207 has 8.00 MiB memory in use. Process 369532 has 8.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 369813 has 8.00 MiB memory in use. Process 370128 has 8.00 MiB memory in use. Process 370299 has 8.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371263 has 8.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371831 has 8.00 MiB memory in use. Process 371872 has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 372920 has 8.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373808 has 8.00 MiB memory in use. Process 373888 has 8.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 20.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/hospital/Easy/P008/image_right/000031_right.png../../datasets/AsymKD/TartanAir/hospital/Easy/P008/depth_right/000031_right_depth.npy
CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 3.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359689 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359692 has 8.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360657 has 8.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361352 has 8.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 362714 has 8.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Including non-PyTorch memory, this process has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364238 has 8.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365660 has 8.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 365899 has 8.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368526 has 8.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369207 has 8.00 MiB memory in use. Process 369532 has 8.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 369813 has 8.00 MiB memory in use. Process 370128 has 8.00 MiB memory in use. Process 370299 has 8.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371263 has 8.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371831 has 8.00 MiB memory in use. Process 371872 has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 372920 has 8.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373808 has 8.00 MiB memory in use. Process 373888 has 8.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 20.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P005/image_left/000546_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P005/depth_left/000546_left_depth.npy
CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 3.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359689 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359692 has 8.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360657 has 8.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361352 has 8.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 362714 has 8.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364238 has 8.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365660 has 8.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368526 has 8.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369207 has 8.00 MiB memory in use. Process 369532 has 8.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 370128 has 8.00 MiB memory in use. Process 370299 has 8.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371263 has 8.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371831 has 8.00 MiB memory in use. Process 371872 has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373808 has 8.00 MiB memory in use. Process 373888 has 8.00 MiB memory in use. Including non-PyTorch memory, this process has 166.00 MiB memory in use. Of the allocated memory 20.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/endofworld/Hard/P000/image_right/000011_right.png../../datasets/AsymKD/TartanAir/endofworld/Hard/P000/depth_right/000011_right_depth.npy
CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 3.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359689 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359692 has 8.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360657 has 8.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361352 has 8.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 362714 has 8.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364238 has 8.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365660 has 8.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368526 has 8.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369532 has 8.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 370128 has 8.00 MiB memory in use. Process 370299 has 8.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Including non-PyTorch memory, this process has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371263 has 8.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371831 has 8.00 MiB memory in use. Process 371872 has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373808 has 8.00 MiB memory in use. Process 373888 has 8.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/BlendedMVS/586d07869d1b5e34c2842e5b/blended_images/00000100_masked.jpg../../datasets/AsymKD/BlendedMVS/586d07869d1b5e34c2842e5b/rendered_depth_maps/00000100.pfm
CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 3.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360657 has 8.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364238 has 8.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Including non-PyTorch memory, this process has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365660 has 8.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368526 has 8.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369532 has 8.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 370299 has 8.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371263 has 8.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371872 has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373808 has 8.00 MiB memory in use. Process 373888 has 8.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/BlendedMVS/5af02e904c8216544b4ab5a2/blended_images/00000187_masked.jpg../../datasets/AsymKD/BlendedMVS/5af02e904c8216544b4ab5a2/rendered_depth_maps/00000187.pfm
CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 3.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360657 has 8.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364238 has 8.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365660 has 8.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368526 has 8.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369532 has 8.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 370299 has 8.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371872 has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Including non-PyTorch memory, this process has 166.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373808 has 8.00 MiB memory in use. Process 373888 has 8.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 20.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/gascola/Hard/P006/image_right/000780_right.png../../datasets/AsymKD/TartanAir/gascola/Hard/P006/depth_right/000780_right_depth.npy
CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 3.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360657 has 8.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Including non-PyTorch memory, this process has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364238 has 8.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365660 has 8.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368526 has 8.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369532 has 8.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371872 has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373808 has 8.00 MiB memory in use. Process 373888 has 8.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/seasonforest_winter/Easy/P001/image_left/000434_left.png../../datasets/AsymKD/TartanAir/seasonforest_winter/Easy/P001/depth_left/000434_left_depth.npy
CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 3.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360657 has 8.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364238 has 8.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365660 has 8.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Including non-PyTorch memory, this process has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368526 has 8.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369532 has 8.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371872 has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373808 has 8.00 MiB memory in use. Process 373888 has 8.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 20.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/office2/Hard/P002/image_left/000688_left.png../../datasets/AsymKD/TartanAir/office2/Hard/P002/depth_left/000688_left_depth.npy
CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 3.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360657 has 8.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365660 has 8.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369532 has 8.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Including non-PyTorch memory, this process has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373888 has 8.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 20.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/HRWSI/train/imgs/114589_ItaliaVaticanPlaceSt-Pierre_W3D.jpg../../datasets/AsymKD/HRWSI/train/gts/114589_ItaliaVaticanPlaceSt-Pierre_W3D.png
CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 3.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Including non-PyTorch memory, this process has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371872 has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/office2/Hard/P003/image_right/001283_right.png../../datasets/AsymKD/TartanAir/office2/Hard/P003/depth_right/001283_right_depth.npy
CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 3.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Including non-PyTorch memory, this process has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371872 has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/gascola/Easy/P004/image_left/001836_left.png../../datasets/AsymKD/TartanAir/gascola/Easy/P004/depth_left/001836_left_depth.npy
CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 3.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Including non-PyTorch memory, this process has 166.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371872 has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 20.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/BlendedMVS/586b0e219d1b5e34c2828862/blended_images/00000122_masked.jpg../../datasets/AsymKD/BlendedMVS/586b0e219d1b5e34c2828862/rendered_depth_maps/00000122.pfm
CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 3.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Including non-PyTorch memory, this process has 166.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371872 has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 20.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/seasonforest_winter/Hard/P011/image_left/000610_left.png../../datasets/AsymKD/TartanAir/seasonforest_winter/Hard/P011/depth_left/000610_left_depth.npy
CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 3.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Including non-PyTorch memory, this process has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371872 has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/office/Hard/P007/image_left/000875_left.png../../datasets/AsymKD/TartanAir/office/Hard/P007/depth_left/000875_left_depth.npy
CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 3.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Including non-PyTorch memory, this process has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371872 has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 20.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/oldtown/Easy/P002/image_left/000112_left.png../../datasets/AsymKD/TartanAir/oldtown/Easy/P002/depth_left/000112_left_depth.npy
CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 3.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Including non-PyTorch memory, this process has 166.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371872 has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 20.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/oldtown/Hard/P002/image_left/000035_left.png../../datasets/AsymKD/TartanAir/oldtown/Hard/P002/depth_left/000035_left_depth.npy
CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 3.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Including non-PyTorch memory, this process has 178.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371872 has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 19.17 MiB is allocated by PyTorch, and 12.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P011/image_left/002258_left.png../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P011/depth_left/002258_left_depth.npy
CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 3.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Including non-PyTorch memory, this process has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371872 has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 20.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/office/Easy/P003/image_right/001581_right.png../../datasets/AsymKD/TartanAir/office/Easy/P003/depth_right/001581_right_depth.npy
CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 7.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 166.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Including non-PyTorch memory, this process has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371872 has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372880 has 166.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373809 has 166.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/abandonedfactory_night/Hard/P008/image_left/000491_left.png../../datasets/AsymKD/TartanAir/abandonedfactory_night/Hard/P008/depth_left/000491_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/carwelding/Easy/P001/image_left/000542_left.png../../datasets/AsymKD/TartanAir/carwelding/Easy/P001/depth_left/000542_left_depth.npy
CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 11.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 178.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371872 has 166.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372880 has 178.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Including non-PyTorch memory, this process has 166.00 MiB memory in use. Of the allocated memory 20.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P011/image_left/001768_left.png../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P011/depth_left/001768_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory_night/Hard/P000/image_right/000755_right.png../../datasets/AsymKD/TartanAir/abandonedfactory_night/Hard/P000/depth_right/000755_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/5b558a928bbfb62204e77ba2/blended_images/00000026_masked.jpg../../datasets/AsymKD/BlendedMVS/5b558a928bbfb62204e77ba2/rendered_depth_maps/00000026.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P006/image_right/000609_right.png../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P006/depth_right/000609_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory_night/Easy/P008/image_right/001462_right.png../../datasets/AsymKD/TartanAir/abandonedfactory_night/Easy/P008/depth_right/001462_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/gascola/Easy/P003/image_left/000997_left.png../../datasets/AsymKD/TartanAir/gascola/Easy/P003/depth_left/000997_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P008/image_right/001046_right.png../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P008/depth_right/001046_right_depth.npy
CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 11.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Including non-PyTorch memory, this process has 178.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371872 has 178.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Process 372880 has 178.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373809 has 178.00 MiB memory in use. Of the allocated memory 32.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/office2/Hard/P003/image_right/001399_right.png../../datasets/AsymKD/TartanAir/office2/Hard/P003/depth_right/001399_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/58524a080e7012308944bcbf/blended_images/00000059_masked.jpg../../datasets/AsymKD/BlendedMVS/58524a080e7012308944bcbf/rendered_depth_maps/00000059.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/westerndesert/Easy/P001/image_left/000280_left.png../../datasets/AsymKD/TartanAir/westerndesert/Easy/P001/depth_left/000280_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/5b558a928bbfb62204e77ba2/blended_images/00000386_masked.jpg../../datasets/AsymKD/BlendedMVS/5b558a928bbfb62204e77ba2/rendered_depth_maps/00000386.pfm
CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 23.62 MiB is free. Process 345419 has 224.00 MiB memory in use. Process 347272 has 200.00 MiB memory in use. Process 349244 has 224.00 MiB memory in use. Process 351184 has 224.00 MiB memory in use. Process 352154 has 200.00 MiB memory in use. Process 353081 has 224.00 MiB memory in use. Process 354092 has 200.00 MiB memory in use. Process 356917 has 166.00 MiB memory in use. Process 358816 has 8.00 MiB memory in use. Process 358815 has 2.97 GiB memory in use. Process 358819 has 8.00 MiB memory in use. Process 358822 has 8.00 MiB memory in use. Process 359691 has 146.00 MiB memory in use. Process 359851 has 178.00 MiB memory in use. Process 360697 has 146.00 MiB memory in use. Process 360741 has 224.00 MiB memory in use. Process 360739 has 198.00 MiB memory in use. Process 361866 has 224.00 MiB memory in use. Process 361706 has 146.00 MiB memory in use. Process 362715 has 146.00 MiB memory in use. Process 362874 has 146.00 MiB memory in use. Process 363275 has 224.00 MiB memory in use. Process 363684 has 248.00 MiB memory in use. Process 363602 has 166.00 MiB memory in use. Process 363605 has 146.00 MiB memory in use. Process 364611 has 166.00 MiB memory in use. Process 364613 has 146.00 MiB memory in use. Process 364610 has 146.00 MiB memory in use. Process 365204 has 224.00 MiB memory in use. Process 365577 has 146.00 MiB memory in use. Process 365578 has 248.00 MiB memory in use. Process 366588 has 200.00 MiB memory in use. Process 366547 has 146.00 MiB memory in use. Process 366668 has 200.00 MiB memory in use. Process 366867 has 248.00 MiB memory in use. Process 367517 has 248.00 MiB memory in use. Process 367558 has 166.00 MiB memory in use. Process 367560 has 146.00 MiB memory in use. Process 367837 has 224.00 MiB memory in use. Process 368249 has 248.00 MiB memory in use. Process 368528 has 146.00 MiB memory in use. Process 368805 has 166.00 MiB memory in use. Process 369613 has 146.00 MiB memory in use. Process 370541 has 146.00 MiB memory in use. Process 370823 has 146.00 MiB memory in use. Process 370902 has 166.00 MiB memory in use. Process 371548 has 146.00 MiB memory in use. Process 371872 has 198.00 MiB memory in use. Process 372147 has 198.00 MiB memory in use. Process 372519 has 146.00 MiB memory in use. Including non-PyTorch memory, this process has 178.00 MiB memory in use. Process 373489 has 146.00 MiB memory in use. Process 373809 has 178.00 MiB memory in use. Of the allocated memory 32.00 MiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) ../../datasets/AsymKD/TartanAir/oldtown/Hard/P008/image_left/001087_left.png../../datasets/AsymKD/TartanAir/oldtown/Hard/P008/depth_left/001087_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/586326ad712e276146904571/blended_images/00000031_masked.jpg../../datasets/AsymKD/BlendedMVS/586326ad712e276146904571/rendered_depth_maps/00000031.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/westerndesert/Hard/P004/image_right/000276_right.png../../datasets/AsymKD/TartanAir/westerndesert/Hard/P004/depth_right/000276_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/5bfc9d5aec61ca1dd69132a2/blended_images/00000215_masked.jpg../../datasets/AsymKD/BlendedMVS/5bfc9d5aec61ca1dd69132a2/rendered_depth_maps/00000215.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/000000000000000000000001/blended_images/00000059_masked.jpg../../datasets/AsymKD/BlendedMVS/000000000000000000000001/rendered_depth_maps/00000059.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/5bea87f4abd34c35e1860ab5/blended_images/00000149_masked.jpg../../datasets/AsymKD/BlendedMVS/5bea87f4abd34c35e1860ab5/rendered_depth_maps/00000149.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/586922da9d1b5e34c2809ff3/blended_images/00000166_masked.jpg../../datasets/AsymKD/BlendedMVS/586922da9d1b5e34c2809ff3/rendered_depth_maps/00000166.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/office/Easy/P006/image_right/000813_right.png../../datasets/AsymKD/TartanAir/office/Easy/P006/depth_right/000813_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/59f37f74b45be2233001ba18/blended_images/00000010_masked.jpg../../datasets/AsymKD/BlendedMVS/59f37f74b45be2233001ba18/rendered_depth_maps/00000010.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/gascola/Easy/P007/image_left/001221_left.png../../datasets/AsymKD/TartanAir/gascola/Easy/P007/depth_left/001221_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/5aa0f478a9efce63548c1cb4/blended_images/00000138_masked.jpg../../datasets/AsymKD/BlendedMVS/5aa0f478a9efce63548c1cb4/rendered_depth_maps/00000138.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/5aaadd4cbc13235570d178a7/blended_images/00000177_masked.jpg../../datasets/AsymKD/BlendedMVS/5aaadd4cbc13235570d178a7/rendered_depth_maps/00000177.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory_night/Easy/P011/image_left/001794_left.png../../datasets/AsymKD/TartanAir/abandonedfactory_night/Easy/P011/depth_left/001794_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/gascola/Easy/P003/image_right/001207_right.png../../datasets/AsymKD/TartanAir/gascola/Easy/P003/depth_right/001207_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/seasonforest_winter/Easy/P005/image_right/001716_right.png../../datasets/AsymKD/TartanAir/seasonforest_winter/Easy/P005/depth_right/001716_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/hospital/Easy/P032/image_left/000797_left.png../../datasets/AsymKD/TartanAir/hospital/Easy/P032/depth_left/000797_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/585bbe55c49c8507c3ce81cd/blended_images/00000287_masked.jpg../../datasets/AsymKD/BlendedMVS/585bbe55c49c8507c3ce81cd/rendered_depth_maps/00000287.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/5a9e5df65baeef72b4a021cd/blended_images/00000182_masked.jpg../../datasets/AsymKD/BlendedMVS/5a9e5df65baeef72b4a021cd/rendered_depth_maps/00000182.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/office2/Easy/P009/image_left/001434_left.png../../datasets/AsymKD/TartanAir/office2/Easy/P009/depth_left/001434_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/ocean/Easy/P011/image_right/000394_right.png../../datasets/AsymKD/TartanAir/ocean/Easy/P011/depth_right/000394_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/5950206a41b158666ac50506/blended_images/00000192_masked.jpg../../datasets/AsymKD/BlendedMVS/5950206a41b158666ac50506/rendered_depth_maps/00000192.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory_night/Hard/P010/image_right/000170_right.png../../datasets/AsymKD/TartanAir/abandonedfactory_night/Hard/P010/depth_right/000170_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/seasonforest_winter/Easy/P005/image_left/001373_left.png../../datasets/AsymKD/TartanAir/seasonforest_winter/Easy/P005/depth_left/001373_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory_night/Hard/P005/image_left/000496_left.png../../datasets/AsymKD/TartanAir/abandonedfactory_night/Hard/P005/depth_left/000496_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/hospital/Easy/P000/image_right/000222_right.png../../datasets/AsymKD/TartanAir/hospital/Easy/P000/depth_right/000222_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/hospital/Easy/P025/image_right/000015_right.png../../datasets/AsymKD/TartanAir/hospital/Easy/P025/depth_right/000015_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/hospital/Easy/P015/image_right/000349_right.png../../datasets/AsymKD/TartanAir/hospital/Easy/P015/depth_right/000349_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/5aa0f478a9efce63548c1cb4/blended_images/00000036_masked.jpg../../datasets/AsymKD/BlendedMVS/5aa0f478a9efce63548c1cb4/rendered_depth_maps/00000036.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/HRWSI/train/imgs/108271_W3D_MontreuxQuaiFleurs.jpg../../datasets/AsymKD/HRWSI/train/gts/108271_W3D_MontreuxQuaiFleurs.png
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/HRWSI/train/imgs/63815_z98_Plate-formePetroliere.jpg../../datasets/AsymKD/HRWSI/train/gts/63815_z98_Plate-formePetroliere.png
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/carwelding/Easy/P002/image_left/000875_left.png../../datasets/AsymKD/TartanAir/carwelding/Easy/P002/depth_left/000875_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory_night/Hard/P003/image_left/000782_left.png../../datasets/AsymKD/TartanAir/abandonedfactory_night/Hard/P003/depth_left/000782_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory_night/Easy/P007/image_right/000347_right.png../../datasets/AsymKD/TartanAir/abandonedfactory_night/Easy/P007/depth_right/000347_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory_night/Easy/P005/image_left/000401_left.png../../datasets/AsymKD/TartanAir/abandonedfactory_night/Easy/P005/depth_left/000401_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/hospital/Hard/P046/image_left/000121_left.png../../datasets/AsymKD/TartanAir/hospital/Hard/P046/depth_left/000121_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/hospital/Hard/P047/image_left/000515_left.png../../datasets/AsymKD/TartanAir/hospital/Hard/P047/depth_left/000515_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/5a7cb1d6fe5c0d6fb53e64fb/blended_images/00000434_masked.jpg../../datasets/AsymKD/BlendedMVS/5a7cb1d6fe5c0d6fb53e64fb/rendered_depth_maps/00000434.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/seasidetown/Easy/P009/image_left/000728_left.png../../datasets/AsymKD/TartanAir/seasidetown/Easy/P009/depth_left/000728_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/seasonforest_winter/Hard/P010/image_right/000658_right.png../../datasets/AsymKD/TartanAir/seasonforest_winter/Hard/P010/depth_right/000658_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/gascola/Easy/P008/image_right/000963_right.png../../datasets/AsymKD/TartanAir/gascola/Easy/P008/depth_right/000963_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P008/image_left/000889_left.png../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P008/depth_left/000889_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/seasonforest_winter/Easy/P005/image_left/001279_left.png../../datasets/AsymKD/TartanAir/seasonforest_winter/Easy/P005/depth_left/001279_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/HRWSI/train/imgs/68894_pbz98_3D_MPO_70pc.jpg../../datasets/AsymKD/HRWSI/train/gts/68894_pbz98_3D_MPO_70pc.png
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/seasonforest_winter/Easy/P004/image_right/000123_right.png../../datasets/AsymKD/TartanAir/seasonforest_winter/Easy/P004/depth_right/000123_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/585369770e7012308945c709/blended_images/00000158_masked.jpg../../datasets/AsymKD/BlendedMVS/585369770e7012308945c709/rendered_depth_maps/00000158.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P004/image_right/000858_right.png../../datasets/AsymKD/TartanAir/abandonedfactory/Easy/P004/depth_right/000858_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/seasonforest_winter/Easy/P002/image_right/000194_right.png../../datasets/AsymKD/TartanAir/seasonforest_winter/Easy/P002/depth_right/000194_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/westerndesert/Hard/P004/image_right/000243_right.png../../datasets/AsymKD/TartanAir/westerndesert/Hard/P004/depth_right/000243_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/seasonforest/Easy/P007/image_right/000330_right.png../../datasets/AsymKD/TartanAir/seasonforest/Easy/P007/depth_right/000330_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/gascola/Hard/P005/image_right/000819_right.png../../datasets/AsymKD/TartanAir/gascola/Hard/P005/depth_right/000819_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/seasidetown/Easy/P004/image_right/000350_right.png../../datasets/AsymKD/TartanAir/seasidetown/Easy/P004/depth_right/000350_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/endofworld/Easy/P005/image_right/000656_right.png../../datasets/AsymKD/TartanAir/endofworld/Easy/P005/depth_right/000656_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/amusement/Hard/P000/image_right/000285_right.png../../datasets/AsymKD/TartanAir/amusement/Hard/P000/depth_right/000285_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/endofworld/Easy/P003/image_right/000390_right.png../../datasets/AsymKD/TartanAir/endofworld/Easy/P003/depth_right/000390_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/office/Hard/P005/image_left/000599_left.png../../datasets/AsymKD/TartanAir/office/Hard/P005/depth_left/000599_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/seasidetown/Easy/P008/image_right/000523_right.png../../datasets/AsymKD/TartanAir/seasidetown/Easy/P008/depth_right/000523_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/BlendedMVS/5895f2329a8c0314c5d00117/blended_images/00000022_masked.jpg../../datasets/AsymKD/BlendedMVS/5895f2329a8c0314c5d00117/rendered_depth_maps/00000022.pfm
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/carwelding/Hard/P002/image_right/000106_right.png../../datasets/AsymKD/TartanAir/carwelding/Hard/P002/depth_right/000106_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/ocean/Hard/P007/image_right/000532_right.png../../datasets/AsymKD/TartanAir/ocean/Hard/P007/depth_right/000532_right_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/office/Easy/P001/image_left/000217_left.png../../datasets/AsymKD/TartanAir/office/Easy/P001/depth_left/000217_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/abandonedfactory/Hard/P003/image_left/000746_left.png../../datasets/AsymKD/TartanAir/abandonedfactory/Hard/P003/depth_left/000746_left_depth.npy
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
 ../../datasets/AsymKD/TartanAir/gascola/Hard/P003/image_left/000115_left.png../../datasets/AsymKD/TartanAir/gascola/Hard/P003/depth_left/000115_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
Dimension out of range (expected to be in range of [-2, 1], but got 2)
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000471_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000471_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000467_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000467_left_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/office/Hard/P001/image_right/000471_right.png../../datasets/AsymKD/TartanAir/office/Hard/P001/depth_right/000471_right_depth.npy
'NoneType' object has no attribute 'shape' ../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/image_left/000474_left.png../../datasets/AsymKD/TartanAir/japanesealley/Easy/P007/depth_left/000474_left_depth.npy
index -1 is out of bounds for axis 0 with size 0 ../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/blended_images/00000008_masked.jpg../../datasets/AsymKD/BlendedMVS/00000000000000000000000f/rendered_depth_maps/00000008.pfm
